{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring tokenization for classification with BERT and RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we explore how BERT and RoBERTa models tokenize texts or pairs of texts, and what special tokens are aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with tokenizing a single text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "example_text = [\"Here is some text to encode\"]\n",
    "encoded_input = tokenizer(example_text, return_tensors='pt')\n",
    "for key, value in encoded_input.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get three different things for each text:\n",
    "- `input_ids` - The indices for the corresponding tokens.\n",
    "- `attention_mask` - This is for the Transformer, to mask out any padding token and prevent it from being involved in the calculations! Since we only have one text, there is no need to use padding to get all texts to be of the same length. Thus, all tokens have the mask set to `True`.\n",
    "- `token_type_ids` - This was seen during class. As BERT was prepared to receive texts pairs of texts, they have one embedding for each text in the pair. This is indicated by the `token_type_id`. In this case, we have one text only, and thus it is set to `0` in all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token with ID 101, corresponding to '[CLS]' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2182, corresponding to 'here' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2003, corresponding to 'is' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2070, corresponding to 'some' - attention_mask=1, token_type_id=0\n",
      "Token with ID 3793, corresponding to 'text' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2000, corresponding to 'to' - attention_mask=1, token_type_id=0\n",
      "Token with ID 4372, corresponding to 'en' - attention_mask=1, token_type_id=0\n",
      "Token with ID 16044, corresponding to '##code' - attention_mask=1, token_type_id=0\n",
      "Token with ID 102, corresponding to '[SEP]' - attention_mask=1, token_type_id=0\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_input['input_ids'][0]\n",
    "token_type_ids = encoded_input['token_type_ids'][0]\n",
    "attention_mask = encoded_input['attention_mask'][0]\n",
    "\n",
    "for input_id, token_type_id, attention_mask in zip(input_ids, token_type_ids, attention_mask):\n",
    "    print(f\"Token with ID {input_id}, corresponding to '{tokenizer.decode(input_id)}' - attention_mask={attention_mask.item()}, token_type_id={token_type_id.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After investigating each token further, we can see the the `[CLS]` and `[SEP]` tokens seen in class were added. Let's now check how pairs of texts are tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102, 2182, 2003, 2070, 2062, 3793, 2000, 4372, 16044, 102]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "example_text = [(\"Here is some text to encode\", \"Here is some more text to encode\")]\n",
    "encoded_input = tokenizer(example_text, return_tensors='pt')\n",
    "for key, value in encoded_input.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token with ID 101, corresponding to '[CLS]' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2182, corresponding to 'here' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2003, corresponding to 'is' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2070, corresponding to 'some' - attention_mask=1, token_type_id=0\n",
      "Token with ID 3793, corresponding to 'text' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2000, corresponding to 'to' - attention_mask=1, token_type_id=0\n",
      "Token with ID 4372, corresponding to 'en' - attention_mask=1, token_type_id=0\n",
      "Token with ID 16044, corresponding to '##code' - attention_mask=1, token_type_id=0\n",
      "Token with ID 102, corresponding to '[SEP]' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2182, corresponding to 'here' - attention_mask=1, token_type_id=1\n",
      "Token with ID 2003, corresponding to 'is' - attention_mask=1, token_type_id=1\n",
      "Token with ID 2070, corresponding to 'some' - attention_mask=1, token_type_id=1\n",
      "Token with ID 2062, corresponding to 'more' - attention_mask=1, token_type_id=1\n",
      "Token with ID 3793, corresponding to 'text' - attention_mask=1, token_type_id=1\n",
      "Token with ID 2000, corresponding to 'to' - attention_mask=1, token_type_id=1\n",
      "Token with ID 4372, corresponding to 'en' - attention_mask=1, token_type_id=1\n",
      "Token with ID 16044, corresponding to '##code' - attention_mask=1, token_type_id=1\n",
      "Token with ID 102, corresponding to '[SEP]' - attention_mask=1, token_type_id=1\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_input['input_ids'][0]\n",
    "token_type_ids = encoded_input['token_type_ids'][0]\n",
    "attention_mask = encoded_input['attention_mask'][0]\n",
    "\n",
    "for input_id, token_type_id, attention_mask in zip(input_ids, token_type_ids, attention_mask):\n",
    "    print(f\"Token with ID {input_id}, corresponding to '{tokenizer.decode(input_id)}' - attention_mask={attention_mask.item()}, token_type_id={token_type_id.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after each text in the pair the `[SEP]` token was added. Further, each token of the first text has `token_type_id=0`, while all tokens of the second text have `token_type_id=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa\n",
    "We do the same with RoBERTa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[0, 11773, 16, 103, 2788, 7, 46855, 2, 2, 11773, 16, 103, 55, 2788, 7, 46855, 2]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "example_text = [(\"Here is some text to encode\", \"Here is some more text to encode\")]\n",
    "encoded_input = tokenizer(example_text, return_tensors='pt')\n",
    "for key, value in encoded_input.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token with ID 0, corresponding to '<s>' - attention_mask=1\n",
      "Token with ID 11773, corresponding to 'Here' - attention_mask=1\n",
      "Token with ID 16, corresponding to ' is' - attention_mask=1\n",
      "Token with ID 103, corresponding to ' some' - attention_mask=1\n",
      "Token with ID 2788, corresponding to ' text' - attention_mask=1\n",
      "Token with ID 7, corresponding to ' to' - attention_mask=1\n",
      "Token with ID 46855, corresponding to ' encode' - attention_mask=1\n",
      "Token with ID 2, corresponding to '</s>' - attention_mask=1\n",
      "Token with ID 2, corresponding to '</s>' - attention_mask=1\n",
      "Token with ID 11773, corresponding to 'Here' - attention_mask=1\n",
      "Token with ID 16, corresponding to ' is' - attention_mask=1\n",
      "Token with ID 103, corresponding to ' some' - attention_mask=1\n",
      "Token with ID 55, corresponding to ' more' - attention_mask=1\n",
      "Token with ID 2788, corresponding to ' text' - attention_mask=1\n",
      "Token with ID 7, corresponding to ' to' - attention_mask=1\n",
      "Token with ID 46855, corresponding to ' encode' - attention_mask=1\n",
      "Token with ID 2, corresponding to '</s>' - attention_mask=1\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_input['input_ids'][0]\n",
    "attention_mask = encoded_input['attention_mask'][0]\n",
    "\n",
    "for input_id, attention_mask in zip(input_ids, attention_mask):\n",
    "    print(f\"Token with ID {input_id}, corresponding to '{tokenizer.decode(input_id)}' - attention_mask={attention_mask.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see two things:\n",
    "1. RoBERTa does not use `token_type_ids`, they do not add these new different embeddings for the text pairs.\n",
    "2. Instead of `[CLS]` and `[SEP]`, they add `<s>` and `</s>`. We can use `<s>` as we would use `[CLS]`, since they both appear as the first token always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing parameters of a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/mdl-dl_nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to be able to freeze any layers of the pre-trained transformer during fine-tuning. To do this, we just need to set `param.requires_grad = False` for the correct parameters. Unfortunately, each model may be structured in a different way. Thus, we need to inspect the model we will be using to learn how to access the correct modules for freezing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the full module list of the loaded model:\n",
    "1. `embeddings` - This module holds the token embeddings, the positional embeddings and the different embeddings for the first and second texts.\n",
    "2. `encoder` - The transformer itself, with a list of 12 identical transformer layers.\n",
    "3. `pooler` - An additional layer provided by HuggingFace that pools the sequence of embeddings to a single text embedding. **We will ignore this, as we want to have full control over how we pool the sequence and get the final logits or probabilities**. If you get a model that has already trained for the task you need, you will want to reuse their `pooler` and final layers, as they are already trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can access to each module separately to freeze a specific number of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frozen_layers = 5\n",
    "# Freeze the first `num_frozen_layers` layers of the model\n",
    "for layer in bert_model.encoder.layer[:num_frozen_layers]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "# Freeze initial embeddings\n",
    "for param in bert_model.embeddings.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can study the modules of the RoBERTa model, and see how we need to access them in order to freeze the desired parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model = AutoModel.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "roberta_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has the same structure as BERT! The same code will freeze the parameters correctly. Let's create a method for freezing layers of BERT-based models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers_for_bert_based_models(model, num_frozen_layers):\n",
    "    # Freeze the first `num_frozen_layers` layers of the model\n",
    "    for layer in model.encoder.layer[:num_frozen_layers]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    # Freeze initial embeddings\n",
    "    for param in model.embeddings.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for a simple text classification task\n",
    "Let's now try to fine-tune a model for a simple text classification task: classifying movie reviews as either positive or negative. We will use a pre-trained model with one unfrozen layer only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torchmetrics import Accuracy\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with some simple data loading code, which uses the tokenizer to transform the texts into batched tensors of `input_ids`, `attention_mask` and possibly `token_type_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFn():\n",
    "    def __init__(self, tokenizer_name, max_length):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        texts = [example['text'] for example in batch]\n",
    "        labels = [example['label'] for example in batch]\n",
    "\n",
    "        encoded_input = self.tokenizer(\n",
    "            texts,\n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        return encoded_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer_name, max_length, batch_size):\n",
    "        super().__init__()\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "\n",
    "        trainval_dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "        test_dataset = dataset['test']\n",
    "\n",
    "        self.train, self.val = trainval_dataset['train'], trainval_dataset['test']\n",
    "        self.test = test_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=CollateFn(self.tokenizer_name, self.max_length),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=CollateFn(self.tokenizer_name, self.max_length),\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=CollateFn(self.tokenizer_name, self.max_length),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get to PytorchLightning module, where we add together a pre-trained transformer and a classification head to produce the final prediction. We also pass as parameters:\n",
    "- `pooling` - The mechanism used to reduce token embeddings to a single text embedding to make the prediction. We can pass `cls` to take the first token, or `mean` to take the average over all tokens. **But be careful!** If you take the mean, make sure to not pick the padding tokens, by using the `attention_mask`.\n",
    "- `frozen_layers` - The amount of layers of the pre-trained model we want to freeze during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextBinaryClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name, optimizer_params, pooling=\"mean\", frozen_layers=0):\n",
    "        \"\"\"\n",
    "        model_name: The name of the model to use\n",
    "        optimizer_params: Parameters to pass to the optimizer\n",
    "        pooling: The pooling strategy to use. Either 'cls' or 'mean'\n",
    "        frozen_layers: The number of layers to freeze in the pre-trained model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size, 1)  # Binary classification\n",
    "        freeze_layers_for_bert_based_models(self.model, frozen_layers)\n",
    "        \n",
    "        assert pooling in [\"cls\", \"mean\"], \"Pooling must be either 'cls' or 'mean'\"\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.accuracy = Accuracy(task=\"binary\")\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # input_ids: (batch_size, seq_length)\n",
    "        # attention_mask: (batch_size, seq_length)\n",
    "\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state  # (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        if self.pooling == \"cls\":\n",
    "            # NOTE Option 1: Use the CLS token\n",
    "            pool_output = last_hidden_state[:, 0, :]  # (batch_size, hidden_size)\n",
    "        else:\n",
    "            # NOTE Option 2: Use the mean of all tokens\n",
    "            mean_coeffs = attention_mask.float() / attention_mask.float().sum(dim=1, keepdim=True)  # (batch_size, seq_length)\n",
    "            pool_output = torch.einsum(\"bld,bl->bd\", last_hidden_state, mean_coeffs)  # (batch_size, hidden_size)\n",
    "\n",
    "        logits = self.classifier(pool_output)  # (batch_size, 1)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self._step(batch)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log('train_accuracy', accuracy, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self._step(batch)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val_accuracy', accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self._step(batch)\n",
    "        self.log('test_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('test_accuracy', accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def _step(self, batch):\n",
    "        encoded_input, labels = batch\n",
    "        labels = labels.float().view(-1, 1)\n",
    "\n",
    "        logits = self(**encoded_input)  # (batch_size, 1)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "        accuracy = self.accuracy(logits, labels)\n",
    "        \n",
    "        return loss, accuracy\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), **self.optimizer_params)\n",
    "        return optimizer\n",
    "    \n",
    "    def configure_callbacks(self):\n",
    "        return super().configure_callbacks() + [\n",
    "            pl.callbacks.ModelCheckpoint(monitor='val_loss', mode='min'),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# MODEL = \"bert-base-uncased\"\n",
    "MODEL = \"FacebookAI/roberta-base\"\n",
    "POOLING = \"mean\"  # [\"cls\", \"mean\"]\n",
    "NUM_FROZEN_LAYERS = 11  # Leave one layer unfrozen\n",
    "OPTIMIZER_PARAMS = {\n",
    "    'lr': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/home/pablo/.micromamba/envs/mdl-dl_nlp/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/amp.py:54: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pablo/.micromamba/envs/mdl-dl_nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "data_module = IMDBDataModule(MODEL, MAX_LENGTH, BATCH_SIZE)\n",
    "model = TextBinaryClassifier(MODEL, OPTIMIZER_PARAMS, POOLING, NUM_FROZEN_LAYERS)\n",
    "\n",
    "data_module.setup()\n",
    "trainer = pl.Trainer(max_epochs=3, accelerator=\"gpu\", devices=[0], precision=\"16-mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | model      | RobertaModel   | 124 M \n",
      "1 | classifier | Linear         | 769   \n",
      "2 | accuracy   | BinaryAccuracy | 0     \n",
      "----------------------------------------------\n",
      "7.7 M     Trainable params\n",
      "116 M     Non-trainable params\n",
      "124 M     Total params\n",
      "498.586   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/mdl-dl_nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/mdl-dl_nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 176/176 [01:18<00:00,  2.24it/s, v_num=17, train_loss_step=0.270, train_accuracy_step=0.910, val_loss=0.173, val_accuracy=0.936, train_loss_epoch=0.188, train_accuracy_epoch=0.929] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 176/176 [01:18<00:00,  2.24it/s, v_num=17, train_loss_step=0.270, train_accuracy_step=0.910, val_loss=0.173, val_accuracy=0.936, train_loss_epoch=0.188, train_accuracy_epoch=0.929]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "Restoring states from the checkpoint path at /home/pablo/classes/MP_DL-DL_NLP/Lecture 2 - Text Classification/lightning_logs/version_17/checkpoints/epoch=1-step=352.ckpt\n",
      "/home/pablo/.micromamba/envs/mdl-dl_nlp/lib/python3.11/site-packages/lightning_fabric/utilities/cloud_io.py:55: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Loaded model weights from the checkpoint at /home/pablo/classes/MP_DL-DL_NLP/Lecture 2 - Text Classification/lightning_logs/version_17/checkpoints/epoch=1-step=352.ckpt\n",
      "/home/pablo/.micromamba/envs/mdl-dl_nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 196/196 [01:04<00:00,  3.03it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.9341999888420105\n",
      "        test_loss           0.17330922186374664\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.17330922186374664, 'test_accuracy': 0.9341999888420105}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(ckpt_path=\"best\", datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! $93.42$% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot text classification with LLMs\n",
    "\n",
    "We now try zero-shot classification with chat-based LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the following prompts, they are more or less self-explanatory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are a sentiment classifier. You classify movie reviews as 'positive' or 'negative'. You only respond with the label.\"\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Classify the sentiment of the following text as 'positive' or 'negative', answering only with the label:\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "SYSTEM_PROMPT = \"You are a sentiment classifier. You classify movie reviews as 'positive' or 'negative'. You only respond with the label.\"\n",
    "```\n",
    "- This is an **instruction** given to the AI model to define its behavior.  \n",
    "- It tells the model that it is a **sentiment classifier** (not a chatbot or general text generator).  \n",
    "- It specifies that the model should only respond with **\"positive\"** or **\"negative\"** (not explanations, extra text, or other responses).  \n",
    "\n",
    "👉 **Purpose:** Ensures the model **stays focused** and gives a **clear, structured response**.\n",
    "\n",
    "```python\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Classify the sentiment of the following text as 'positive' or 'negative', answering only with the label:\n",
    "{text}\n",
    "\"\"\"\n",
    "```\n",
    "- This is a **template** that will be filled with actual text (a movie review) when making a prediction.\n",
    "- The `{text}` part is a **placeholder** that will be replaced with the real review.\n",
    "- The instruction explicitly tells the model to **only output \"positive\" or \"negative\"**.\n",
    "\n",
    "👉 **Purpose:** This ensures that every time we classify a review, we follow a **consistent format** and keep responses structured.\n",
    "\n",
    "**NOTE**: Even though we make it clear that we just want the final answer, the model is probabilistic and can output other things. We need to make sure to parse the answer robustly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "pipe.tokenizer.pad_token_id = pipe.tokenizer.eos_token_id\n",
    "pipe.model.generation_config.pad_token_id = pipe.tokenizer.pad_token_id\n",
    "pipe.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Pipeline**\n",
    "```python\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "```\n",
    "- **`pipeline(\"text-generation\", ...)`**  \n",
    "  - This creates a **text generation pipeline** that makes it easy to use the model for generating text.  \n",
    "  - `\"text-generation\"` tells Hugging Face that we want to **generate text** from a given input.  \n",
    "\n",
    "- **`model=MODEL_ID`**  \n",
    "  - This tells the pipeline which model to use (the one we defined earlier).  \n",
    "\n",
    "- **`torch_dtype=torch.bfloat16`**  \n",
    "  - This sets the data type for the model's computations.  \n",
    "  - `bfloat16` is a **reduced-precision** floating point type that makes the model run **faster and use less memory**, especially on GPUs.  \n",
    "\n",
    "- **`device_map=\"cuda:0\"`**  \n",
    "  - This tells the pipeline to run the model on a **GPU** (specifically, the first GPU, which is `\"cuda:0\"`).  \n",
    "\n",
    "**Setting Padding and Tokenizer Behavior**\n",
    "```python\n",
    "pipe.tokenizer.pad_token_id = pipe.tokenizer.eos_token_id\n",
    "```\n",
    "- The **\"pad token\"** is used when input sequences need to be of the same length (for batch processing).  \n",
    "- Some models, like Llama, **don’t have a predefined pad token**, so this line **sets the pad token to be the same as the EOS (end of sequence) token**.  \n",
    "\n",
    "```python\n",
    "pipe.model.generation_config.pad_token_id = pipe.tokenizer.pad_token_id\n",
    "```\n",
    "- This ensures that the **generation settings** also recognize the pad token correctly.  \n",
    "\n",
    "```python\n",
    "pipe.tokenizer.padding_side = \"left\"\n",
    "```\n",
    "- This tells the tokenizer to **add padding on the left side** of sequences instead of the right.  \n",
    "- The pipeline produces a warning otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset and manually iterate in batches running our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [18:43<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch processing parameters\n",
    "batch_size = 32  # Adjust based on available VRAM\n",
    "total, correct = 0, 0\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "    # Extract batch\n",
    "    batch = test_dataset[i : i + batch_size]\n",
    "    labels = batch[\"label\"]\n",
    "    user_prompts = [PROMPT_TEMPLATE.format(text=text) for text in batch[\"text\"]]\n",
    "\n",
    "    # Create messages in batch format\n",
    "    messages_batch = [[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ] for prompt in user_prompts]\n",
    "\n",
    "    # Run batch inference\n",
    "    outputs = pipe(messages_batch, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "    # Extract predictions\n",
    "    responses = [out[0][\"generated_text\"][-1][\"content\"] for out in outputs]\n",
    "\n",
    "    # Get predictions from responses. In this case, we assume a positive sentiment if the word \"positive\" is present in the response\n",
    "    predicted_labels = [\"positive\" in response.lower() for response in responses]\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    total += len(labels)\n",
    "    correct += sum(int(pred == label) for pred, label in zip(predicted_labels, labels))\n",
    "\n",
    "# Final accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also not bad I would say, specially without training. We should note two things, however:\n",
    "- This large model was very expensive to run, even in inference mode.\n",
    "- I am actually not sure if the model has seen this data during its extensive procedure, so its difficult to know if the measurement of performance is fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot text classification by sentence similarity with `sentence-transformers`\n",
    "\n",
    "Finally, we get to zero-shot classification by sentence similarity. The basic ideas are:\n",
    "- We have a model (we will use the `sentence-transformers` library) that is trained to embed full texts into a semantic vector space.\n",
    "- In this space, texts are close together if they are semantically similar, that is, close in meaning.\n",
    "- Thus, if a textual description of an arbitrary classification label is close to the text, it likely means that the text belongs to that category.\n",
    "- We can compare the descriptions of all the labels, and pick the one that is closes in the space to the text we want to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SentenceTransformer` class handles most of the heavy lifting for us:\n",
    "- Tokenization\n",
    "- Pooling all token embeddings to a single text embedding.\n",
    "We load a `SentenceTransformer` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\n",
    "    'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    device='cuda:0',\n",
    "    model_kwargs={ \"torch_dtype\": torch.float16, },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take embeddings from each label. We consider three options:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: simple labels\n",
    "labels = [\"negative\", \"positive\"]\n",
    "label_embeddings = model.encode(labels, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: more complex labels\n",
    "labels = [\n",
    "    \"the movie is negative dull boring terrible bad\",\n",
    "    \"the movie is positive good awesome amazing interesting entertaining\",\n",
    "]\n",
    "label_embeddings = model.encode(labels, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:25<00:00, 30.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Option 3: use the training dataset to compute accurate embeddings\n",
    "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "pos_embedding_list = []\n",
    "neg_embedding_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(train_dataset), batch_size)):\n",
    "    batch = train_dataset[i : i + batch_size]\n",
    "    texts = batch[\"text\"]\n",
    "    labels = batch[\"label\"]\n",
    "\n",
    "    # Encode text\n",
    "    text_embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "    # Separate positive and negative examples\n",
    "    for label, embedding in zip(labels, text_embeddings):\n",
    "        if label == 1:\n",
    "            pos_embedding_list.append(embedding)\n",
    "        else:\n",
    "            neg_embedding_list.append(embedding)\n",
    "\n",
    "# The embeddings are averaged to get a single embedding per class\n",
    "# NOTE: after averaging, the embeddings are normalized to have unit norm,\n",
    "#       which is important because the model always outputs normalized embeddings,\n",
    "#       and we must keep the same scale for a fair comparison between the labels\n",
    "\n",
    "mean_pos_embedding = torch.stack(pos_embedding_list).mean(dim=0)\n",
    "pos_embedding = F.normalize(mean_pos_embedding, p=2, dim=-1)\n",
    "\n",
    "mean_neg_embedding = torch.stack(neg_embedding_list).mean(dim=0)\n",
    "neg_embedding = F.normalize(mean_neg_embedding, p=2, dim=-1)\n",
    "\n",
    "# Stack the embeddings\n",
    "label_embeddings = torch.stack([neg_embedding, pos_embedding])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method classifies the given text embeddings based on the given label embeddings. So, how do we measure similarity? The model is trained to measure cosine similarity, which is the dot product between the normalized vectors. That's why we normalized earlier! Now we only need to compute the dot products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text_embeddings, label_embeddings):\n",
    "    # text_embeddings: (num_samples, embedding_dim)\n",
    "    # label_embeddings: (num_labels, embedding_dim)\n",
    "    scores = text_embeddings @ label_embeddings.T  # (num_samples, num_labels)\n",
    "    return scores.argmax(dim=1)  # (num_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:24<00:00, 31.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch processing parameters\n",
    "batch_size = 32  # Adjust based on available VRAM\n",
    "total, correct = 0, 0\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "    batch = test_dataset[i : i + batch_size]\n",
    "    texts = batch[\"text\"]\n",
    "    labels = batch[\"label\"]\n",
    "\n",
    "    # Encode text\n",
    "    text_embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "    # Predict labels\n",
    "    predicted_labels = predict(text_embeddings, label_embeddings)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    total += len(labels)\n",
    "    correct += sum(int(pred == label) for pred, label in zip(predicted_labels, labels))\n",
    "\n",
    "\n",
    "# Final accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
