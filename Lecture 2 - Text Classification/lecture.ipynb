{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring tokenization for classification with BERT and RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we explore how BERT and RoBERTa models tokenize texts or pairs of texts, and what special tokens are aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with tokenizing a single text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "example_text = [\"Here is some text to encode\"]\n",
    "encoded_input = tokenizer(example_text, return_tensors='pt')\n",
    "for key, value in encoded_input.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get three different things for each text:\n",
    "- `input_ids` - The indices for the corresponding tokens.\n",
    "- `attention_mask` - This is for the Transformer, to mask out any padding token and prevent it from being involved in the calculations! Since we only have one text, there is no need to use padding to get all texts to be of the same length. Thus, all tokens have the mask set to `True`.\n",
    "- `token_type_ids` - This was seen during class. As BERT was prepared to receive texts pairs of texts, they have one embedding for each text in the pair. This is indicated by the `token_type_id`. In this case, we have one text only, and thus it is set to `0` in all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token with ID 101, corresponding to '[CLS]' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2182, corresponding to 'here' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2003, corresponding to 'is' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2070, corresponding to 'some' - attention_mask=1, token_type_id=0\n",
      "Token with ID 3793, corresponding to 'text' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2000, corresponding to 'to' - attention_mask=1, token_type_id=0\n",
      "Token with ID 4372, corresponding to 'en' - attention_mask=1, token_type_id=0\n",
      "Token with ID 16044, corresponding to '##code' - attention_mask=1, token_type_id=0\n",
      "Token with ID 102, corresponding to '[SEP]' - attention_mask=1, token_type_id=0\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_input['input_ids'][0]\n",
    "token_type_ids = encoded_input['token_type_ids'][0]\n",
    "attention_mask = encoded_input['attention_mask'][0]\n",
    "\n",
    "for input_id, token_type_id, attention_mask in zip(input_ids, token_type_ids, attention_mask):\n",
    "    print(f\"Token with ID {input_id}, corresponding to '{tokenizer.decode(input_id)}' - attention_mask={attention_mask.item()}, token_type_id={token_type_id.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After investigating each token further, we can see the the `[CLS]` and `[SEP]` tokens seen in class were added. Let's now check how pairs of texts are tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102, 2182, 2003, 2070, 2062, 3793, 2000, 4372, 16044, 102]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "example_text = [(\"Here is some text to encode\", \"Here is some more text to encode\")]\n",
    "encoded_input = tokenizer(example_text, return_tensors='pt')\n",
    "for key, value in encoded_input.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token with ID 101, corresponding to '[CLS]' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2182, corresponding to 'here' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2003, corresponding to 'is' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2070, corresponding to 'some' - attention_mask=1, token_type_id=0\n",
      "Token with ID 3793, corresponding to 'text' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2000, corresponding to 'to' - attention_mask=1, token_type_id=0\n",
      "Token with ID 4372, corresponding to 'en' - attention_mask=1, token_type_id=0\n",
      "Token with ID 16044, corresponding to '##code' - attention_mask=1, token_type_id=0\n",
      "Token with ID 102, corresponding to '[SEP]' - attention_mask=1, token_type_id=0\n",
      "Token with ID 2182, corresponding to 'here' - attention_mask=1, token_type_id=1\n",
      "Token with ID 2003, corresponding to 'is' - attention_mask=1, token_type_id=1\n",
      "Token with ID 2070, corresponding to 'some' - attention_mask=1, token_type_id=1\n",
      "Token with ID 2062, corresponding to 'more' - attention_mask=1, token_type_id=1\n",
      "Token with ID 3793, corresponding to 'text' - attention_mask=1, token_type_id=1\n",
      "Token with ID 2000, corresponding to 'to' - attention_mask=1, token_type_id=1\n",
      "Token with ID 4372, corresponding to 'en' - attention_mask=1, token_type_id=1\n",
      "Token with ID 16044, corresponding to '##code' - attention_mask=1, token_type_id=1\n",
      "Token with ID 102, corresponding to '[SEP]' - attention_mask=1, token_type_id=1\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_input['input_ids'][0]\n",
    "token_type_ids = encoded_input['token_type_ids'][0]\n",
    "attention_mask = encoded_input['attention_mask'][0]\n",
    "\n",
    "for input_id, token_type_id, attention_mask in zip(input_ids, token_type_ids, attention_mask):\n",
    "    print(f\"Token with ID {input_id}, corresponding to '{tokenizer.decode(input_id)}' - attention_mask={attention_mask.item()}, token_type_id={token_type_id.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after each text in the pair the `[SEP]` token was added. Further, each token of the first text has `token_type_id=0`, while all tokens of the second text have `token_type_id=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa\n",
    "We do the same with RoBERTa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[0, 11773, 16, 103, 2788, 7, 46855, 2, 2, 11773, 16, 103, 55, 2788, 7, 46855, 2]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "example_text = [(\"Here is some text to encode\", \"Here is some more text to encode\")]\n",
    "encoded_input = tokenizer(example_text, return_tensors='pt')\n",
    "for key, value in encoded_input.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token with ID 0, corresponding to '<s>' - attention_mask=1\n",
      "Token with ID 11773, corresponding to 'Here' - attention_mask=1\n",
      "Token with ID 16, corresponding to ' is' - attention_mask=1\n",
      "Token with ID 103, corresponding to ' some' - attention_mask=1\n",
      "Token with ID 2788, corresponding to ' text' - attention_mask=1\n",
      "Token with ID 7, corresponding to ' to' - attention_mask=1\n",
      "Token with ID 46855, corresponding to ' encode' - attention_mask=1\n",
      "Token with ID 2, corresponding to '</s>' - attention_mask=1\n",
      "Token with ID 2, corresponding to '</s>' - attention_mask=1\n",
      "Token with ID 11773, corresponding to 'Here' - attention_mask=1\n",
      "Token with ID 16, corresponding to ' is' - attention_mask=1\n",
      "Token with ID 103, corresponding to ' some' - attention_mask=1\n",
      "Token with ID 55, corresponding to ' more' - attention_mask=1\n",
      "Token with ID 2788, corresponding to ' text' - attention_mask=1\n",
      "Token with ID 7, corresponding to ' to' - attention_mask=1\n",
      "Token with ID 46855, corresponding to ' encode' - attention_mask=1\n",
      "Token with ID 2, corresponding to '</s>' - attention_mask=1\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_input['input_ids'][0]\n",
    "attention_mask = encoded_input['attention_mask'][0]\n",
    "\n",
    "for input_id, attention_mask in zip(input_ids, attention_mask):\n",
    "    print(f\"Token with ID {input_id}, corresponding to '{tokenizer.decode(input_id)}' - attention_mask={attention_mask.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see two things:\n",
    "1. RoBERTa does not use `token_type_ids`, they do not add these new different embeddings for the text pairs.\n",
    "2. Instead of `[CLS]` and `[SEP]`, they add `<s>` and `</s>`. We can use `<s>` as we would use `[CLS]`, since they both appear as the first token always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing parameters of a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to be able to freeze any layers of the pre-trained transformer during fine-tuning. To do this, we just need to set `param.requires_grad = False` for the correct parameters. Unfortunately, each model may be structured in a different way. Thus, we need to inspect the model we will be using to learn how to access the correct modules for freezing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the full module list of the loaded model:\n",
    "1. `embeddings` - This module holds the token embeddings, the positional embeddings and the different embeddings for the first and second texts.\n",
    "2. `encoder` - The transformer itself, with a list of 12 identical transformer layers.\n",
    "3. `pooler` - An additional layer provided by HuggingFace that pools the sequence of embeddings to a single text embedding. **We will ignore this, as we want to have full control over how we pool the sequence and get the final logits or probabilities**. If you get a model that has already trained for the task you need, you will want to reuse their `pooler` and final layers, as they are already trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can access to each module separately to freeze a specific number of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frozen_layers = 5\n",
    "# Freeze the first `num_frozen_layers` layers of the model\n",
    "for layer in bert_model.encoder.layer[:num_frozen_layers]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "# Freeze initial embeddings\n",
    "for param in bert_model.embeddings.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can study the modules of the RoBERTa model, and see how we need to access them in order to freeze the desired parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model = AutoModel.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "roberta_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has the same structure as BERT! The same code will freeze the parameters correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for a simple text classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = [\"Here is some text to encode\"]\n",
    "encoded_input = bert_tokenizer(example_text, return_tensors='pt')\n",
    "output = bert_model(**encoded_input)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot text classification with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot text classification with sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
