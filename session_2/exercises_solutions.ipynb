{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CQ1. Apply a token classification or NER pipeline to identify named entities in the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/mdl-dl_nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the appropriate pipeline to obtain the expected results below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"dslim/bert-base-NER\"\n",
    "ner_pipe = pipeline('ner', model=MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.99889094,\n",
       "  'index': 4,\n",
       "  'word': 'Pablo',\n",
       "  'start': 11,\n",
       "  'end': 16},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9995328,\n",
       "  'index': 10,\n",
       "  'word': 'Madrid',\n",
       "  'start': 32,\n",
       "  'end': 38},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9991641,\n",
       "  'index': 15,\n",
       "  'word': 'Mu',\n",
       "  'start': 58,\n",
       "  'end': 60},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9830967,\n",
       "  'index': 16,\n",
       "  'word': '##rcia',\n",
       "  'start': 60,\n",
       "  'end': 64}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"My name is Pablo, and I live in Madrid although I am from Murcia\"\n",
    "result = ner_pipe(example)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how multiple tokens can actually correspond to the same named entity! You will need to handle the output to make it more intuitive. Lukily, you are also given the start and end characters, so you can easily join adjacent intervals and take the appropriate characters from the initial string. In this case we find a `B-LOC` entity in character ranges `[58,60)` and `[60,64)`, they should be joined together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CQ2. Fine-tune a RoBERTa model for Natural Language Inference (NLI) on the SNLI dataset\n",
    "\n",
    "**Natural Language Inference (NLI)** is the task of determining the logical relationship between a pair of sentences—typically labeled as **entailment**, **contradiction**, or **neutral**. Given a **premise** (a statement) and a **hypothesis** (another statement), the goal is to predict whether the hypothesis is true (entailment), false (contradiction), or undetermined (neutral) based on the premise.  \n",
    "\n",
    "**The SNLI Dataset (Stanford Natural Language Inference)** is a widely used benchmark dataset for NLI. It consists of 570,000 sentence pairs labeled for entailment, contradiction, and neutral relationships.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/mdl-dl_nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torchmetrics import Accuracy\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers_for_bert_based_models(model, num_frozen_layers):\n",
    "    # Freeze the first `num_frozen_layers` layers of the model\n",
    "    for layer in model.encoder.layer[:num_frozen_layers]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    # Freeze initial embeddings\n",
    "    for param in model.embeddings.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main change over basic text classification is the tokenization. Instead of tokenizing texts, you tokenize pairs of texts! Complete the following `CollateFn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFn():\n",
    "    def __init__(self, tokenizer_name, max_length):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        premises = [example['premise'] for example in batch]\n",
    "        hypothesis = [example['hypothesis'] for example in batch]\n",
    "\n",
    "        texts = list(zip(premises, hypothesis))  # List of pairs of premises and hypothesis\n",
    "        labels = [example['label'] for example in batch]\n",
    "\n",
    "        encoded_input = self.tokenizer(\n",
    "            texts,\n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        return encoded_input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LighningDataModule` is very basic, so no need to complete anything here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer_name, max_length, batch_size):\n",
    "        super().__init__()\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = load_dataset(\"snli\")\n",
    "\n",
    "        filter_missing_labels = lambda example: example['label'] != -1\n",
    "        self.train = dataset['train'].filter(filter_missing_labels)\n",
    "        self.val = dataset['validation'].filter(filter_missing_labels)\n",
    "        self.test = dataset['test'].filter(filter_missing_labels)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=CollateFn(self.tokenizer_name, self.max_length),\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=CollateFn(self.tokenizer_name, self.max_length),\n",
    "            num_workers=4,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=CollateFn(self.tokenizer_name, self.max_length),\n",
    "            num_workers=4,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to complete the following `LightningModule` code. It is basically an adaptation from the one seen during the lecture, except now its a multiclass problem with three labels (contradiction, neutral and entailment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNLIClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name, optimizer_params, pooling=\"mean\", frozen_layers=0):\n",
    "        \"\"\"\n",
    "        model_name: The name of the model to use\n",
    "        optimizer_params: Parameters to pass to the optimizer\n",
    "        pooling: The pooling strategy to use. Either 'cls' or 'mean'\n",
    "        frozen_layers: The number of layers to freeze in the pre-trained model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size, 3)  # Contradiction, Neutral, Entailment\n",
    "        freeze_layers_for_bert_based_models(self.model, frozen_layers)\n",
    "        \n",
    "        assert pooling in [\"cls\", \"mean\"], \"Pooling must be either 'cls' or 'mean'\"\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # input_ids: (batch_size, seq_length)\n",
    "        # attention_mask: (batch_size, seq_length)\n",
    "\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state  # (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        if self.pooling == \"cls\":\n",
    "            # NOTE Option 1: Use the CLS token\n",
    "            pool_output = last_hidden_state[:, 0, :]  # (batch_size, hidden_size)\n",
    "        else:\n",
    "            # NOTE Option 2: Use the mean of all tokens\n",
    "            mean_coeffs = attention_mask.float() / attention_mask.float().sum(dim=1, keepdim=True)  # (batch_size, seq_length)\n",
    "            pool_output = torch.einsum(\"bld,bl->bd\", last_hidden_state, mean_coeffs)  # (batch_size, hidden_size)\n",
    "\n",
    "        logits = self.classifier(pool_output)  # (batch_size, 3)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self._step(batch)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log('train_accuracy', accuracy, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self._step(batch)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val_accuracy', accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self._step(batch)\n",
    "        self.log('test_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('test_accuracy', accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def _step(self, batch):\n",
    "        encoded_input, labels = batch\n",
    "\n",
    "        logits = self(**encoded_input)  # (batch_size, 3)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        accuracy = self.accuracy(logits, labels)\n",
    "        \n",
    "        return loss, accuracy\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), **self.optimizer_params)\n",
    "        return optimizer\n",
    "    \n",
    "    def configure_callbacks(self):\n",
    "        return super().configure_callbacks() + [\n",
    "            pl.callbacks.ModelCheckpoint(monitor='val_loss', mode='min'),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can train the model, modifying any parameters you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# MODEL = \"bert-base-uncased\"\n",
    "MODEL = \"FacebookAI/roberta-base\"\n",
    "POOLING = \"mean\"  # [\"cls\", \"mean\"]\n",
    "NUM_FROZEN_LAYERS = 10  # Leave two layers unfrozen\n",
    "OPTIMIZER_PARAMS = {\n",
    "    'lr': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "data_module = SNLIDataModule(MODEL, MAX_LENGTH, BATCH_SIZE)\n",
    "model = TextNLIClassifier(MODEL, OPTIMIZER_PARAMS, POOLING, NUM_FROZEN_LAYERS)\n",
    "\n",
    "data_module.setup()\n",
    "trainer = pl.Trainer(max_epochs=2, accelerator=\"gpu\", devices=[1], precision=\"16-mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | model      | RobertaModel       | 124 M \n",
      "1 | classifier | Linear             | 2.3 K \n",
      "2 | accuracy   | MulticlassAccuracy | 0     \n",
      "--------------------------------------------------\n",
      "14.8 M    Trainable params\n",
      "109 M     Non-trainable params\n",
      "124 M     Total params\n",
      "498.592   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 4292/4292 [04:46<00:00, 14.96it/s, v_num=28, train_loss_step=0.498, train_accuracy_step=0.790, val_loss=0.358, val_accuracy=0.864, train_loss_epoch=0.453, train_accuracy_epoch=0.822]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 4292/4292 [04:48<00:00, 14.87it/s, v_num=28, train_loss_step=0.498, train_accuracy_step=0.790, val_loss=0.358, val_accuracy=0.864, train_loss_epoch=0.453, train_accuracy_epoch=0.822]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "Restoring states from the checkpoint path at /home/pablo/classes/MP_DL-DL_NLP/Lecture 2 - Text Classification/lightning_logs/version_28/checkpoints/epoch=1-step=8584.ckpt\n",
      "/home/pablo/.micromamba/envs/mdl-dl_nlp/lib/python3.11/site-packages/lightning_fabric/utilities/cloud_io.py:55: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Loaded model weights from the checkpoint at /home/pablo/classes/MP_DL-DL_NLP/Lecture 2 - Text Classification/lightning_logs/version_28/checkpoints/epoch=1-step=8584.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 77/77 [00:03<00:00, 21.65it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.8611563444137573\n",
      "        test_loss           0.36514949798583984\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.36514949798583984, 'test_accuracy': 0.8611563444137573}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(ckpt_path=\"best\", datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
